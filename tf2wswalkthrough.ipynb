{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall numpy -y\n",
    "!pip install numpy\n",
    "!pip install tensorflow==2.0.0-beta1 --ignore-installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure TensorFlow is version 2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train = X_train[:55000]\n",
    "X_train = X_train.reshape(X_train.shape[0],X_train.shape[1]*X_train.shape[2]).T\n",
    "\n",
    "Y_train = Y_train[:55000]\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train).T\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1]*X_test.shape[2]).T\n",
    "\n",
    "print(\"number of training examples = \" + str(X_train.shape[1]))\n",
    "print(\"number of test examples = \" + str(X_test.shape[1]))\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF2 runs in eager execution by default. Disable this to run the TF 1.X code from the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download TensorFlow  1.X code shown. \n",
    "TF2 provides a tility to automatically update code to TF 2.0. Run that upgrade script, print its output, and copy and paste into a cell to run. See post for scripting details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/PubChimps/tf2/master/tf1.py >> tf1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### paste upgraded code here\n",
    "n_input = 784  \n",
    "n_classes = 10  \n",
    "dropout = 1  \n",
    "num_epochs = 60\n",
    "X = tf.compat.v1.placeholder(tf.float32, [n_input, None])\n",
    "Y = tf.compat.v1.placeholder(tf.float32, [n_classes, None])\n",
    "W1 = tf.compat.v1.get_variable(\"W1\", [64,n_input], initializer = tf.compat.v1.random_normal_initializer)\n",
    "b1 = tf.compat.v1.get_variable(\"b1\", [64,1], initializer = tf.compat.v1.zeros_initializer())\n",
    "W2 = tf.compat.v1.get_variable(\"W2\", [32,64], initializer = tf.compat.v1.random_normal_initializer)\n",
    "b2 = tf.compat.v1.get_variable(\"b2\", [32,1], initializer = tf.compat.v1.zeros_initializer())\n",
    "W3 = tf.compat.v1.get_variable(\"W3\", [10,32], initializer = tf.compat.v1.random_normal_initializer)\n",
    "b3 = tf.compat.v1.get_variable(\"b3\", [10,1], initializer = tf.compat.v1.zeros_initializer())\n",
    "\n",
    "Z1 = tf.add(tf.matmul(W1,X), b1)                      \n",
    "A1 = tf.nn.relu(Z1)                                   \n",
    "Z2 = tf.add(tf.matmul(W2,A1), b2) \n",
    "A2 = tf.nn.relu(Z2) \n",
    "Z3 = tf.add(tf.matmul(W3,A2), b3) \n",
    "\n",
    "cost = tf.reduce_mean(input_tensor=tf.nn.softmax_cross_entropy_with_logits(logits=tf.transpose(a=Z3), labels=tf.stop_gradient(tf.transpose(a=Y))))\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=.01).minimize(cost)\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        _ , epoch_cost = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train})\n",
    "        if epoch % 20 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "        \n",
    " \n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(input=Z3), tf.argmax(input=Y))\n",
    "    accuracy = tf.reduce_mean(input_tensor=tf.cast(correct_prediction, \"float\"))\n",
    "    print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once TensorFlow code is run, trying to re-enable eager execution (which is the default runtime in TF 2.0) should throw the error `ValueError: tf.enable_eager_execution must be called at program startup.` \n",
    "To continue select 'Kernel' at the top of Watson Studio and Restart. Then skip the cell above and move to `import tensorflow...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in TF 1.X are two ways to run Keras code in TF2.0, eagerly and as a graph, both presented below. The first model trains in Python. Notice how the second, which uses AutoGraph, takes a while to get started running (as TensorFlow converts and optimizes a computaional graph), but then trains much quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_mnist_features_and_labels(x, y):\n",
    "    x = tf.cast(x, tf.float32) / 255.0\n",
    "    y = tf.cast(y, tf.int64)\n",
    "    return x, y\n",
    "\n",
    "def mnist_dataset():\n",
    "    (x, y), _ = tf.keras.datasets.mnist.load_data()\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    ds = ds.map(prepare_mnist_features_and_labels)\n",
    "    ds = ds.take(20000).shuffle(20000).batch(100)\n",
    "    return ds\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.build()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "compute_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = compute_loss(y, logits)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    compute_accuracy(y, logits)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train(model, optimizer):\n",
    "    train_ds = mnist_dataset()\n",
    "    step = 0\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for x, y in train_ds:\n",
    "        step += 1\n",
    "        loss = train_one_step(model, optimizer, x, y)\n",
    "        if tf.equal(step % 10, 0):\n",
    "            tf.print('Step', step, ': loss', loss, '; accuracy', compute_accuracy.result())\n",
    "    return step, loss, accuracy\n",
    "\n",
    "step, loss, accuracy = train(model, optimizer)\n",
    "print('Final step', step, ': loss', loss, '; accuracy', compute_accuracy.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
